{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3189ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', 4000)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c68f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('/fastcampus-data/articles/articels_only_contents.json')\n",
    "df = df1.copy()\n",
    "df[\"full_content\"] = df['title'] + ' ' + df['content_tag_removed']\n",
    "df = df[['id', 'title', 'full_content']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e21a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = '8e216d4f-bfae-4cc4-ae6f-25f2dc4968c5' # 혜빈 키\n",
    "api_key = 'de384842-9f74-42c9-a0e5-036ecd76eab5' # 영수님 키"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae699f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // 언어 분석 기술 문어/구어 중 한가지만 선택해 사용\n",
    "# // 언어 분석 기술(문어)\n",
    "openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\" \n",
    "# // 언어 분석 기술(구어)\n",
    "# openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU_spoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfcfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETRISentenceAnalysis:\n",
    "  def __init__(self, api_key: dict, url: str):\n",
    "    self.api_key = api_key\n",
    "    self.url = url\n",
    "\n",
    "    self.http = urllib3.PoolManager()\n",
    "  \n",
    "  def _make_request_json(self, text: str, analysis_code: int) -> dict:\n",
    "    return {\n",
    "      \"access_key\": self.api_key,\n",
    "      \"argument\": {\n",
    "          \"text\": text,\n",
    "          \"analysis_code\": analysis_code\n",
    "          }\n",
    "      }\n",
    "\n",
    "  def _request(self, request_json: dict) -> object:\n",
    "    return self.http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(request_json)\n",
    "    )\n",
    "\n",
    "  def get_analyzed_sentence(self, sentence: str, analysis_code: int) -> dict:\n",
    "    def _get_return_object(sentence: str) -> dict:\n",
    "      data = str(response.data, 'utf-8')\n",
    "      data = json.loads(data)\n",
    "      \n",
    "       # 딕셔너리 하나 벗김 \n",
    "      return data.get('return_object') \n",
    "\n",
    "    request_json = self._make_request_json(sentence, analysis_code)\n",
    "    response = self._request(request_json)\n",
    "\n",
    "#     assert response.status == 200, f'Error {response.status}'\n",
    "        \n",
    "    return _get_return_object(response)\n",
    "\n",
    "def get_dependency_from_text(data: dict) -> dict:\n",
    "    # 딕셔너리 하나 더 벗김\n",
    "    analyzed_sentences = data.get('sentence')\n",
    "    \n",
    "    extracted_sentences_by_text_and_dependency = {\n",
    "        i: {'text':sentence.get('text'), # 문장 한개씩 뱉음\n",
    "            'dependency': {\n",
    "                'subject': [info.get('text') for info in sentence.get('dependency') if info.get('label') == 'NP' or info.get('label') == 'NP_SBJ'],\n",
    "                'object': [info.get('text') for info in sentence.get('dependency') if info.get('label') == 'NP_OBJ']\n",
    "                }\n",
    "            } for i, sentence in enumerate(analyzed_sentences)\n",
    "        }\n",
    "    \n",
    "    return extracted_sentences_by_text_and_dependency\n",
    "\n",
    "# only nouns\n",
    "def get_nouns_from_text(data: dict) -> dict:\n",
    "    # 딕셔너리 하나 더 벗김\n",
    "    analyzed_sentences = data.get('sentence')\n",
    "    \n",
    "    extracted_sentences_by_text_and_dependency = {\n",
    "        i: {'text':sentence.get('text'), # 문장 한개씩 뱉음\n",
    "            'dependency': [info.get('text') for info in sentence.get('morp') if info.get('type') == 'NNG']\n",
    "            } for i, sentence in enumerate(analyzed_sentences)\n",
    "        }\n",
    "    \n",
    "    return extracted_sentences_by_text_and_dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72a20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "etri_sentence_analysis = ETRISentenceAnalysis(api_key, openApiURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc519a2",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc353e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  한국어를 제외한 글자를 제거하는 패턴.\n",
    "import re    \n",
    "df['full_content'] = df['full_content'].apply(lambda x: re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8cbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['full_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7d4cf",
   "metadata": {},
   "source": [
    "글자가 20글자 이하이거나, 내용이 NoneType인 아티클 제거하기 총 31개. id로 접근함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bcf07c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(id_less_20) =  0\n",
      "id_less_20 =  []\n",
      "len(new_id) =  26\n",
      "new_id =  [15107, 15376, 18326, 17194, 22131, 27831, 27888, 24527, 29398, 29528, 28451, 28471, 30618, 29157, 27366, 27451, 29891, 33399, 32309, 33743, 33446, 34361, 34422, 35145, 35259, 36137]\n",
      "\n",
      "len(drop_id_list) =  26\n",
      "drop_id_list =  [15107, 15376, 18326, 30618, 28451, 33446, 36137, 17194, 32309, 28471, 27831, 34361, 27451, 35259, 29891, 35145, 24527, 33743, 29398, 29528, 29157, 27366, 27888, 22131, 34422, 33399]\n"
     ]
    }
   ],
   "source": [
    "# contents 글자가 20글자 이하인 아티클\n",
    "id_less_20 = []\n",
    "for i in range(len(df['full_content'])):\n",
    "    if len(df['full_content'][i]) < 20:\n",
    "#         print(i, df['full_content'][i])\n",
    "        id_less_20.append(df['id'][i])\n",
    "\n",
    "        \n",
    "print('len(id_less_20) = ',len(id_less_20))    \n",
    "print('id_less_20 = ',id_less_20)    \n",
    "\n",
    "# none type인 아티클\n",
    "type_none_idx = [1723, 1724, 1804, 1826, 2245, 2318, 2319, 2377, 2431, 2442, 2455, 2456, 2542, 2561, 2563, 2567, 2655, 2701, 2768, 2832, 2857, 2937, 2938, 2954, 3065, 3088] \n",
    "new_id = []\n",
    "# for i in range(len(df['content_tag_removed'])):\n",
    "for j in type_none_idx:\n",
    "    new_id.append(df.iloc[j].id)\n",
    "    \n",
    "print('len(new_id) = ',len(new_id))    \n",
    "print('new_id = ', new_id)    \n",
    "print()\n",
    "\n",
    "drop_id_list = list(set(id_less_20 + new_id))\n",
    "print('len(drop_id_list) = ',len(drop_id_list))\n",
    "print('drop_id_list = ', drop_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea647cc1",
   "metadata": {},
   "source": [
    "# corpus에 형태소 분석 결과 담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e3b9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3438/3438 [19:41<00:00,  2.91it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3438"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "#     if df['id'][i] not in drop_id_list:\n",
    "    con = df['full_content'].iloc[i]\n",
    "    # 형태소 분석\n",
    "    data = etri_sentence_analysis.get_analyzed_sentence(con, 'morp')\n",
    "    corpus.append(data)\n",
    "        \n",
    "#3438 - 31 = 3407 이어야 함.        \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c4eaf1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3438/3438 [02:01<00:00, 28.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# dataframe 초기화\n",
    "new_df = pd.DataFrame(df[['id','title','full_content']])\n",
    "new_df['tokens'] = None\n",
    "\n",
    "# contents 20글자 이하인 corpus Pop\n",
    "\n",
    "\n",
    "# dataframe 각 토픽 담기\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    nouns = []\n",
    "    res3 = corpus[i]['sentence'][0]['morp'] # 형태소 결과\n",
    "    \n",
    "    for j in res3:\n",
    "        if j['type'] == 'NNG' and len(j['lemma']) > 1: #  형태소가 명사이고 2글자 이상인 것만\n",
    "#             nouns.insert(i, j['lemma'])\n",
    "            nouns.append(j['lemma'])\n",
    "            new_df['tokens'].iloc[i] = nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4a073",
   "metadata": {},
   "source": [
    "# save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f045a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set으로 토큰 중복제거\n",
    "# new_df['tokens'] = new_df['tokens'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5c37318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.to_json('/home/user_4/CBF/Token/final_tokens_article.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
