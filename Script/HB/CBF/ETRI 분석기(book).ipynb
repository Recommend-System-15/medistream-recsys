{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33ad1d5",
   "metadata": {},
   "source": [
    "* 형태소 분석 (문어/구어) : \"morp\", -> 사용할 단위\n",
    "* 어휘의미 분석 (동음이의어 분석)(문어) : \"wsd\"\n",
    "* 어휘의미 분석 (다의어 분석)(문어) : \"wsd_poly\"\n",
    "* 개체명 인식 (문어/구어) : \"ner\"\n",
    "* 의존 구문 분석 (문어) : \"dparse\"\n",
    "* 의미역 인식 (문어) : \"srl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120ee71",
   "metadata": {},
   "source": [
    "# upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec545bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3\n",
    "import json\n",
    "\n",
    "df1 = pd.read_json('/home/user_1/medistream-recsys/Script/YS/df_book_clean.json')\n",
    "df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ccb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '8e216d4f-bfae-4cc4-ae6f-25f2dc4968c5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777e836",
   "metadata": {},
   "source": [
    "언어 분석 기술 문어/구어 중 한가지만 선택해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e76128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // 언어 분석 기술(문어)\n",
    "openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\" \n",
    "# // 언어 분석 기술(구어)\n",
    "# openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU_spoken\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213cefd3",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec50f5c",
   "metadata": {},
   "source": [
    "### 1. api로 불러온 data에서 필요한 정보 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfcfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETRISentenceAnalysis:\n",
    "    def __init__(self, api_key: dict, url: str):\n",
    "        self.api_key = api_key\n",
    "        self.url = url\n",
    "\n",
    "        self.http = urllib3.PoolManager()\n",
    "  \n",
    "    def _make_request_json(self, text: str, analysis_code: int) -> dict:\n",
    "        return {\n",
    "          \"access_key\": self.api_key,\n",
    "          \"argument\": {\n",
    "              \"text\": text,\n",
    "              \"analysis_code\": analysis_code\n",
    "              }\n",
    "          }\n",
    "\n",
    "    def _request(self, request_json: dict) -> object:\n",
    "        return self.http.request(\n",
    "            \"POST\",\n",
    "            openApiURL,\n",
    "            headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "            body=json.dumps(request_json)\n",
    "        )\n",
    "\n",
    "    def get_analyzed_sentence(self, sentence: str, analysis_code: int) -> dict:\n",
    "        def _get_return_object(sentence: str) -> dict:\n",
    "            data = str(response.data, 'utf-8')\n",
    "            data = json.loads(data)\n",
    "\n",
    "\n",
    "            return data.get('return_object') \n",
    "\n",
    "        request_json = self._make_request_json(sentence, analysis_code)\n",
    "        response = self._request(request_json)\n",
    "\n",
    "        assert response.status == 200, f'Error {response.status}'\n",
    "\n",
    "        return _get_return_object(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3065dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_from_text(data: dict) -> dict:\n",
    "    analyzed_sentences = data.get('sentence')\n",
    "    \n",
    "    extracted_sentences_by_text_and_dependency = {\n",
    "        i: {'text':sentence.get('text'), # 문장 한개씩 뱉음\n",
    "            'dependency': {\n",
    "                'subject': [info.get('text') for info in sentence.get('dependency') if info.get('label') == 'NP' or info.get('label') == 'NP_SBJ'],\n",
    "                'object': [info.get('text') for info in sentence.get('dependency') if info.get('label') == 'NP_OBJ']\n",
    "                }\n",
    "            } for i, sentence in enumerate(analyzed_sentences)\n",
    "        }\n",
    "    \n",
    "    return extracted_sentences_by_text_and_dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f77a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only nouns\n",
    "def get_nouns_from_text(data: dict) -> dict:\n",
    "    analyzed_sentences = data.get('sentence')\n",
    "    \n",
    "    extracted_sentences_by_text_and_dependency = {\n",
    "        i: {'text':sentence.get('text'), # 문장 한개씩 뱉음\n",
    "            'dependency': [info.get('text') for info in sentence.get('morp') if info.get('type') == 'NNG']\n",
    "            } for i, sentence in enumerate(analyzed_sentences)\n",
    "        }\n",
    "    \n",
    "    return extracted_sentences_by_text_and_dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72a20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "etri_sentence_analysis = ETRISentenceAnalysis(api_key, openApiURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a9a648",
   "metadata": {},
   "source": [
    "### 2. 1만 글자 이하로 전처리(1만 글자가 넘는 경우 api가 처리하지 못함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270c3a1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-c2177851f8b7>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['description'][df.index == 126] = df[df.index == 126].description.values[0].replace(\" \",'')\n",
      "<ipython-input-4-c2177851f8b7>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['description'][df.index == 229] = df[df.index == 229].description.values[0].replace(\" \",'')\n"
     ]
    }
   ],
   "source": [
    "# 1만 글자 넘는 경우 확인\n",
    "# for i in range(len(df)):\n",
    "#     if len(df['description'].iloc[i]) >= 10000:\n",
    "#         print(df.iloc[i])\n",
    "        \n",
    "# 1만 글자 넘는 것은 띄어쓰기 없애는 전처리 진행\n",
    "df['description'][df.index == 126] = df[df.index == 126].description.values[0].replace(\" \",'')\n",
    "df['description'][df.index == 229] = df[df.index == 229].description.values[0].replace(\" \",'')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f503bae",
   "metadata": {},
   "source": [
    "# corpus에 명사만 담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e61032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'etri_sentence_analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9a44cbed4906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 형태소 분석\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metri_sentence_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_analyzed_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'morp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'etri_sentence_analysis' is not defined"
     ]
    }
   ],
   "source": [
    "# corpus에 형태소 분석 결과 담기\n",
    "\n",
    "corpus = []\n",
    "for i in range(200):\n",
    "    con = df['description'].iloc[i]\n",
    "    # 형태소 분석\n",
    "    data = etri_sentence_analysis.get_analyzed_sentence(con, 'morp')\n",
    "    corpus.append(data)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1200d5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe 초기화\n",
    "new_df = pd.DataFrame(df[['name_x','description']])\n",
    "new_df['tokens'] = None\n",
    "\n",
    "# dataframe 각 토픽 담기\n",
    "for i in range(len(corpus)):\n",
    "    nouns = []\n",
    "    res3 = corpus[i]['sentence'][0]['morp'] # 형태소 결과\n",
    "    \n",
    "    for j in res3:\n",
    "        if j['type'] == 'NNG' and len(j['lemma']) > 1: #  형태소가 명사이고 2글자 이상인 것만\n",
    "            nouns.append(j['lemma'])\n",
    "            new_df['tokens'].iloc[i] = nouns\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dabf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.loc[:199,].to_json('/home/user_4/CBF/tokens_199.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4282468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.loc[200:,].to_json('/home/user_4/CBF/to200frame.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
