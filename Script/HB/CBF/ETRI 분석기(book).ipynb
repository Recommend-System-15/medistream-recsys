{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8120ee71",
   "metadata": {},
   "source": [
    "# ETRI 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec545bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 영수님이 만드신 파일 upload\n",
    "df1 = pd.read_json('/home/user_1/medistream-recsys/Script/YS/df_book_clean.json')\n",
    "df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7a5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62362944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석 (문어/구어) : \"morp\",\n",
    "# 어휘의미 분석 (동음이의어 분석)(문어) : \"wsd\"\n",
    "# 어휘의미 분석 (다의어 분석)(문어) : \"wsd_poly\"\n",
    "# 개체명 인식 (문어/구어) : \"ner\"\n",
    "# 의존 구문 분석 (문어) : \"dparse\"\n",
    "# 의미역 인식 (문어) : \"srl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ccb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '8e216d4f-bfae-4cc4-ae6f-25f2dc4968c5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e76128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // 언어 분석 기술 문어/구어 중 한가지만 선택해 사용\n",
    "# // 언어 분석 기술(문어)\n",
    "openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\" \n",
    "# // 언어 분석 기술(구어)\n",
    "# openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU_spoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfcfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETRISentenceAnalysis:\n",
    "  def __init__(self, api_key: dict, url: str):\n",
    "    self.api_key = api_key\n",
    "    self.url = url\n",
    "\n",
    "    self.http = urllib3.PoolManager()\n",
    "  \n",
    "  def _make_request_json(self, text: str, analysis_code: int) -> dict:\n",
    "    return {\n",
    "      \"access_key\": self.api_key,\n",
    "      \"argument\": {\n",
    "          \"text\": text,\n",
    "          \"analysis_code\": analysis_code\n",
    "          }\n",
    "      }\n",
    "\n",
    "  def _request(self, request_json: dict) -> object:\n",
    "    return self.http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(request_json)\n",
    "    )\n",
    "\n",
    "  def get_analyzed_sentence(self, sentence: str, analysis_code: int) -> dict:\n",
    "    def _get_return_object(sentence: str) -> dict:\n",
    "      data = str(response.data, 'utf-8')\n",
    "      data = json.loads(data)\n",
    "      \n",
    "       # 딕셔너리 하나 벗김 \n",
    "      return data.get('return_object') \n",
    "\n",
    "    request_json = self._make_request_json(sentence, analysis_code)\n",
    "    response = self._request(request_json)\n",
    "\n",
    "    assert response.status == 200, f'Error {response.status}'\n",
    "        \n",
    "    return _get_return_object(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3065dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_from_text(data: dict) -> dict:\n",
    "    # 딕셔너리 하나 더 벗김\n",
    "    analyzed_sentences = data.get('sentence')\n",
    "    \n",
    "    extracted_sentences_by_text_and_dependency = {\n",
    "        i: {'text':sentence.get('text'), # 문장 한개씩 뱉음\n",
    "            'dependency': {\n",
    "                'subject': [info.get('text') for info in sentence.get('dependency') if info.get('label') == 'NP' or info.get('label') == 'NP_SBJ'],\n",
    "                'object': [info.get('text') for info in sentence.get('dependency') if info.get('label') == 'NP_OBJ']\n",
    "                }\n",
    "            } for i, sentence in enumerate(analyzed_sentences)\n",
    "        }\n",
    "    \n",
    "    return extracted_sentences_by_text_and_dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f77a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only nouns\n",
    "def get_nouns_from_text(data: dict) -> dict:\n",
    "    # 딕셔너리 하나 더 벗김\n",
    "    analyzed_sentences = data.get('sentence')\n",
    "    \n",
    "    extracted_sentences_by_text_and_dependency = {\n",
    "        i: {'text':sentence.get('text'), # 문장 한개씩 뱉음\n",
    "            'dependency': [info.get('text') for info in sentence.get('morp') if info.get('type') == 'NNG']\n",
    "            } for i, sentence in enumerate(analyzed_sentences)\n",
    "        }\n",
    "    \n",
    "    return extracted_sentences_by_text_and_dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72a20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "etri_sentence_analysis = ETRISentenceAnalysis(api_key, openApiURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "270c3a1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id                                            5e4ccce44267e105dfff1e99\n",
      "category_id_x                                  5cf8bbba0098b2225c5dfaa3\n",
      "date_created                      {'$date': '2020-02-19T05:51:32.026Z'}\n",
      "name_x                                                         고령자 한방진료\n",
      "description                __   책 소개  의미 있는 구별과 의미 없는 구별의 엄격한 구별도 원리...\n",
      "meta_description_x                                     초고령자 시대의 새로운 임상서\n",
      "meta_title                                                     고령자 한방진료\n",
      "tags_x                                                        [청홍, 지상사]\n",
      "three_months                                                          7\n",
      "regular_price_y                                                 18500.0\n",
      "m_name                                                               전문\n",
      "L_name                                                               도서\n",
      "s_name                                                        [청홍, 지상사]\n",
      "book_corpus           고령자 한방진료,     __   책 소개  의미 있는 구별과 의미 없는 구별의 엄...\n",
      "book_corpus_konlp     [의림, 중의학, 시험, 위해, 구별, 정신, 종합, 수준, 보충, 신기, 계통, ...\n",
      "Name: 126, dtype: object\n",
      "_id                                            60334e0d527223001a209c16\n",
      "category_id_x                                  5cf8bbba0098b2225c5dfaa3\n",
      "date_created                      {'$date': '2021-02-22T06:24:13.352Z'}\n",
      "name_x                                                           운곡본초도감\n",
      "description            한의사이자 한의대 본초학교수이면서 하자가 없는 학문을 연구하는 입장에서 쪽팔리게 ...\n",
      "meta_description_x                                                     \n",
      "meta_title                                                       운곡본초도감\n",
      "tags_x                                                             [우석]\n",
      "three_months                                                          2\n",
      "regular_price_y                                                180000.0\n",
      "m_name                                                               전문\n",
      "L_name                                                               도서\n",
      "s_name                                                             [우석]\n",
      "book_corpus           운곡본초도감, 한의사이자 한의대 본초학교수이면서 하자가 없는 학문을 연구하는 입장에...\n",
      "book_corpus_konlp     [요원, 기적, 망사, 홍화, 지정, 위령선, 참고, 왜곡, 용골, 위해, 대산, ...\n",
      "Name: 229, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-6c0ec43d83d8>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['description'][df.index == 126] = df[df.index == 126].description.values[0].replace(\" \",'')\n",
      "<ipython-input-9-6c0ec43d83d8>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['description'][df.index == 229] = df[df.index == 229].description.values[0].replace(\" \",'')\n"
     ]
    }
   ],
   "source": [
    "# 1만 글자 넘는 경우\n",
    "# for i in range(len(df)):\n",
    "#     if len(df['description'].iloc[i]) >= 10000:\n",
    "#         print(df.iloc[i])\n",
    "        \n",
    "# 1만 글자 넘는 것은 띄어쓰기 없애는 전처리 진행\n",
    "df['description'][df.index == 126] = df[df.index == 126].description.values[0].replace(\" \",'')\n",
    "df['description'][df.index == 229] = df[df.index == 229].description.values[0].replace(\" \",'')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e61032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpus에 형태소 분석 결과 담기\n",
    "# df['description'] = df['description'][99:] # 99번 인덱스부터\n",
    "corpus = []\n",
    "for i in range(200):\n",
    "    con = df['description'].iloc[i]\n",
    "    # 형태소 분석\n",
    "    data = etri_sentence_analysis.get_analyzed_sentence(con, 'morp')\n",
    "    corpus.append(data)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1200d5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe 초기화\n",
    "new_df = pd.DataFrame(df[['name_x','description']])\n",
    "new_df['tokens'] = None\n",
    "\n",
    "# dataframe 각 토픽 담기\n",
    "for i in range(len(corpus)):\n",
    "    nouns = []\n",
    "    res3 = corpus[i]['sentence'][0]['morp'] # 형태소 결과\n",
    "    \n",
    "    for j in res3:\n",
    "        if j['type'] == 'NNG' and len(j['lemma']) > 1: #  형태소가 명사이고 2글자 이상인 것만\n",
    "#             nouns.insert(i, j['lemma'])\n",
    "            nouns.append(j['lemma'])\n",
    "            new_df['tokens'].iloc[i] = nouns\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dabf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.loc[:199,].to_json('/home/user_4/CBF/tokens_199.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4282468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.loc[200:,].to_json('/home/user_4/CBF/to200frame.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
