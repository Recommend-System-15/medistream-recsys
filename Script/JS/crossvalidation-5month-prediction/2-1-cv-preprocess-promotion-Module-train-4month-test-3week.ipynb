{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144c71b0",
   "metadata": {},
   "source": [
    "https://assaeunji.github.io/machine%20learning/2020-11-29-implicitfeedback/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758f1b3",
   "metadata": {},
   "source": [
    "# test 3주 정합성 검증\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbe592",
   "metadata": {},
   "source": [
    "# 추천 모델\n",
    "- ALS MF, LMF, MP (총 3개)\n",
    "- 총 3개의 추천을 진행하며 MF와 LMF 의 경우 콜드스타트 유저(신규 유저)인 경우 MP로 추천 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053cf1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import random\n",
    "import implicit\n",
    "from implicit.als import AlternatingLeastSquares as ALS\n",
    "\n",
    "%cd /home/user_3/medistream-recsys/Script\n",
    "from preprocessing import drop_columns,dict_to_column,dict_to_set,set_to_column,key_to_element\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.sparse as sp\n",
    "from implicit.lmf import LogisticMatrixFactorization as LMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5943a3d",
   "metadata": {},
   "source": [
    "# 1.Dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# products name 확인 용\n",
    "products_df = pd.read_json(\"/fastcampus-data/products/products.json\")\n",
    "products_df = key_to_element(['_id'],products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/fastcampus-data/select_column_version_4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_paid'] = pd.to_datetime(df['date_paid'])\n",
    "all_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e15e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_name_fill(product_name_preprocess_df):\n",
    "    # 각 마지막 product_ids, name으로 채우기\n",
    "    product_ids_to_name = {}\n",
    "    for idx, row in product_name_preprocess_df.iterrows():\n",
    "        product_ids_to_name[row.product_ids] = row.name_x\n",
    "    product_name_preprocess_df['name_x'] = product_name_preprocess_df['product_ids'].apply(lambda x: product_ids_to_name[x])\n",
    "\n",
    "    name_to_product_ids = {}\n",
    "    for idx, row in product_name_preprocess_df.iterrows():\n",
    "        name_to_product_ids[row.name_x] = row.product_ids\n",
    "    product_name_preprocess_df['product_ids'] = product_name_preprocess_df['name_x'].apply(lambda x: name_to_product_ids[x])\n",
    "    return product_name_preprocess_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotion_proprof(df):\n",
    "    from datetime import datetime\n",
    "\n",
    "    preprocessed_book_df_date = df.copy()\n",
    "\n",
    "    promotion_book_df = preprocessed_book_df_date[preprocessed_book_df_date['date_paid'] >= '2022-01-01']\n",
    "    promotion_book_df['date_paid_date'] = promotion_book_df['date_paid'].dt.date\n",
    "    promotion_book_df['date_paid_week'] = promotion_book_df['date_paid_date'].apply(lambda x: x.isocalendar()[1])\n",
    "\n",
    "    promotion_dict = {\n",
    "        2:['트리거포인트 침치료'],\n",
    "        3:['藥徵, 약의 징표','파킨슨병 한의진료','침의 과학적 접근의 이해','길익동동','Medical acupuncture 침의 과학적 접근과 임상활용',\\\n",
    "          '동의보감 약선','수화론(水火論)'],\n",
    "        4:['실전한약가이드','음양승강으로 해석하는 사상의학: 생리병리'],\n",
    "        5:['음양승강으로 해석하는 사상의학: 생리병리'],\n",
    "        6:['윤상훈·권병조의 알짜 근육학','임상 한의사를 위한 기본 한약처방 강의 2판','트리거포인트 침치료','KCD 한방내과 진찰진단 가이드라인',\\\n",
    "          '실전한약가이드','음양승강으로 해석하는 사상의학: 생리병리','藥徵, 약의 징표','증보운곡본초학','통증치료를 위한 근육 초음파와 주사 테크닉'],\n",
    "        7:['오국통 온병명방'],\n",
    "        9:['병태생리 Visual map','NEO 인턴 핸드북','보험한약 브런치 the # 2판 개정판','Kendall 자세와 통증치료에 있어서 근육의 기능과 검사 5판',\\\n",
    "          '사상방 사용설명서','실전한약가이드','일차진료 한의사를 위한 보험한약입문 - 둘째 판','증보운곡본초학'],\n",
    "        10:['한눈에 보는 스트레칭 해부학'],\n",
    "        11:['임산부에게 사용할 수 있는 한방처방'],\n",
    "        12:['임산부에게 사용할 수 있는 한방처방'],\n",
    "        13:['MRI 자신감 키우기_족부편'],\n",
    "        14:['장골의 PI 변위는 없다'],\n",
    "        15:['윤상훈·권병조의 알짜 근육학','임상 한의사를 위한 기본 한약처방 강의 2판','KCD 한방내과 진찰진단 가이드라인','트리거포인트 침치료',\\\n",
    "           '음양승강으로 해석하는 사상의학: 생리병리','침의 과학적 접근의 이해','실전한약가이드','임산부에게 사용할 수 있는 한방처방','한눈에 보는 스트레칭 해부학',\\\n",
    "           'MRI 자신감 키우기_족부편'],\n",
    "        16:['환자상담의 달인','병의원 경영과 자산 관리 클리닉','우리 병원의 문제? 현장에서 답을 찾다!','근육학','스파이랄 및 키네지오 테이핑',\\\n",
    "           '요양병원 주치의 진료핵심'],\n",
    "        17:['오당 본초강론','운동기능장애 치료 매뉴얼','K. 한의학 임상총론','한방 활용 가이드','최강통증매선','암 치료에 이용되는 천연약물',\\\n",
    "           '왕문원 임상 평형침법','중국 왕문원 평형침구학'],\n",
    "        18:['초음파 가이드 근골격계 통증 치료의 정석'],\n",
    "        19:['초음파 가이드 근골격계 통증 치료의 정석','섭혜민 명의경방험안'],\n",
    "        20:['카이로프랙틱 기본테크닉론'],\n",
    "        21:['흔히보는 정형외과 외래진료 가이드북'],\n",
    "        22:['趙紹琴(조소금) 내과학','한의학 상담','숨찬 세상, 호흡기를 편하게',\\\n",
    "         '의학심오(醫學心悟)','안면마비 침구치료','중경서 독법 강해(상,하) /개정판'],\n",
    "        23:['선생님, 이제 그만 저 좀 포기해 주세요','한의학 상담','숨찬 세상, 호흡기를 편하게',\\\n",
    "        '의학심오(醫學心悟)','중경서 독법 강해(상,하) /개정판','안면마비 침구치료'],\n",
    "     24:['황황교수의 임상의를 위한 근거기반 상한금궤 처방 매뉴얼','황황교수의 개원 한의사를 위한 상한금궤 처방 강의록',\\\n",
    "        '선생님, 이제 그만 저 좀 포기해 주세요'],\\\n",
    "     25:['황황교수의 임상의를 위한 근거기반 상한금궤 처방 매뉴얼',\\\n",
    "       '황황교수의 개원 한의사를 위한 상한금궤 처방 강의록','약침의 정석 –통증편','갑상선 진료 완전정복',\\\n",
    "       '신경학 증상의 감별법','이것이 알고싶다! 당뇨병진료','어지럼질환의 진단과 치료','증례와 함께 하는 한약처방',\\\n",
    "       '뇌의학의 첫걸음','HAPPY 소아청소년 진료'],\\\n",
    "     26:['약침의 정석 –통증편','갑상선 진료 완전정복','신경학 증상의 감별법',\\\n",
    "       '증례와 함께 하는 한약처방','이것이 알고싶다! 당뇨병진료','HAPPY 소아청소년 진료','어지럼질환의 진단과 치료',\\\n",
    "       '뇌의학의 첫걸음','실전, 임상한의학 내과질환을 중심으로','실전, 임상한의학 알레르기질환','침구대성','평주온열경위'],\n",
    "     27:['침구과 진료매뉴얼','실전, 임상한의학 내과질환을 중심으로','실전, 임상한의학 알레르기질환','내과학 5권세트','한방순환 신경내과학',\\\n",
    "        '침구대성'],\n",
    "     28:['감별진단의 정석','기본통증진료학','약처방의 정석 (1, 2권 세트)','QBook: Case based Review',\\\n",
    "         'SMART 내과 1권 : 바이탈, 감염, 종양, 류마티스','일차진료아카데미 처방가이드'],\n",
    "     29:['비만문답','사암침의 해석과 임상'],\n",
    "     30:['플로차트 정형외과 진단','침구과 진료매뉴얼','내과학 5권세트','한방순환 신경내과학'],\n",
    "     31:['외래에서 꼭 알아야 할 통증증후군 137가지'],\n",
    "     32:['SMART 기본 일차진료매뉴얼 3판(세트)','SMART 소아진료매뉴얼 3판','SMART 응급진료매뉴얼(세트)'],\n",
    "     33:['SMART 기본 일차진료매뉴얼 3판(세트)','SMART 소아진료매뉴얼 3판','SMART 응급진료매뉴얼(세트)'],\n",
    "     34:['초음파 유도하 침 시술 가이드북'],\n",
    "     35:['영어 진료 가이드북','초음파 유도하 침 시술 가이드북'],\n",
    "     36:['영어 진료 가이드북','소아피부질환해설'],\n",
    "     37:['소아피부질환해설','醫學心悟(의학심오) 톺아보기'],}\n",
    "\n",
    "    promotion_item_list = []\n",
    "    for promotion_items in promotion_dict.values():\n",
    "        for item in promotion_items:\n",
    "            promotion_item_list.append(item)\n",
    "\n",
    "    # set(promotion_item_list), len(set(promotion_item_list))\n",
    "    \n",
    "    preprocess_promotion_df = promotion_book_df[~((promotion_book_df['name_x'].str.contains('침의 과학적 접근과 임상활용')) & \\\n",
    "                            (promotion_book_df['date_paid_week']==3))]\n",
    "    preprocess_promotion_df = preprocess_promotion_df[~((preprocess_promotion_df['name_x'].str.contains('의학심오')) & \\\n",
    "                                (preprocess_promotion_df['date_paid_week']==22))]\n",
    "    preprocess_promotion_df = preprocess_promotion_df[~((preprocess_promotion_df['name_x'].str.contains('의학심오')) & \\\n",
    "                                (preprocess_promotion_df['date_paid_week']==23))]\n",
    "    preprocess_promotion_df = preprocess_promotion_df[~((preprocess_promotion_df['name_x'].str.contains('약처방의 정석')) & \\\n",
    "                                (preprocess_promotion_df['date_paid_week']==28))]\n",
    "    preprocess_promotion_df = preprocess_promotion_df[~((preprocess_promotion_df['name_x'].str.contains('초음파 유도하 침')) & \\\n",
    "                                (preprocess_promotion_df['date_paid_week']==34))]\n",
    "    preprocess_promotion_df = preprocess_promotion_df[~((preprocess_promotion_df['name_x'].str.contains('초음파 유도하 침')) & \\\n",
    "                                (preprocess_promotion_df['date_paid_week']==34))]\n",
    "    preprocess_promotion_df = preprocess_promotion_df[~((preprocess_promotion_df['name_x'].str.contains('영어 진료 가이드북')) & \\\n",
    "                                (preprocess_promotion_df['date_paid_week']==35))]\n",
    "    preprocess_promotion_df = preprocess_promotion_df[~((preprocess_promotion_df['name_x'].str.contains('영어 진료 가이드북')) & \\\n",
    "                                (preprocess_promotion_df['date_paid_week']==36))]\n",
    "    all_promotion_df = preprocess_promotion_df[~((preprocess_promotion_df['name_x'].str.contains('의학심오')) & \\\n",
    "                                (preprocess_promotion_df['date_paid_week']==37))]\n",
    "\n",
    "    for key,value in promotion_dict.items():\n",
    "        all_promotion_df = all_promotion_df[~((all_promotion_df['name_x'].isin(value)) & (all_promotion_df['date_paid_week']==key))]\n",
    "    \n",
    "    return all_promotion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_paid'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_day_list =  []\n",
    "test_day_list = []\n",
    "\n",
    "# 1일 test 날짜 확인\n",
    "last_date_state = '2022-09-13'\n",
    "split_day = pd.to_datetime(last_date_state)-relativedelta(months=4)\n",
    "test_day = pd.to_datetime(last_date_state)\n",
    "'''\n",
    "마지막 날짜에서 개월 수를 자름 -> split_day\n",
    "months 만 바꾸면 21번 분량이 나옴\n",
    "'''\n",
    "'''\n",
    "train validation test\n",
    "5month 3week     3week\n",
    "42day 전부터 자르면 됨\n",
    "'''\n",
    "for i in range(21,42):\n",
    "    sp_day = str((split_day-timedelta(days=i+1)).to_pydatetime().date())\n",
    "    tt_day  = str((test_day-timedelta(days=i+1)).to_pydatetime().date())\n",
    "    split_day_list.append(sp_day)\n",
    "    test_day_list.append(tt_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fdb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvaluator:\n",
    "    # relavence 모두 1로 동일하게 봄\n",
    "    def _idcg(self, l):\n",
    "        return sum((1.0 / np.log(i + 2) for i in range(l)))\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        self._idcgs = [self._idcg(i) for i in range(1000)]\n",
    "    '''\n",
    "    idcgs 예시, item 3개 추천되므로 3.074281787960283 가 됩니다.\n",
    "    [0, 1.4426950408889634, 2.352934267515801, 3.074281787960283]\n",
    "    '''\n",
    "\n",
    "    def _ndcg(self, gt, rec):\n",
    "        dcg = 0.0\n",
    "        for i, r in enumerate(rec):\n",
    "            if r in gt:\n",
    "                dcg += 1.0 / np.log(i + 2)\n",
    "\n",
    "        return dcg / self._idcgs[len(gt)]\n",
    "    \n",
    "    def _entropy_diversity(self,rec_list):\n",
    "        import six\n",
    "        import math\n",
    "        \n",
    "        topn = len(rec_list[0]['items'])\n",
    "        users = [i.get('id',None) for i in rec_list]\n",
    "        sz = float(len(users)) * topn\n",
    "        freq = {}\n",
    "        for rec in rec_list:\n",
    "            for r in rec['items']:\n",
    "                freq[r] = freq.get(r, 0) + 1\n",
    "        ent = -sum([v / sz * math.log(v / sz) for v in six.itervalues(freq)])\n",
    "        return ent\n",
    "\n",
    "    def _eval(self, gt_list, rec_list):\n",
    "        gt_dict = {g[\"id\"]: g for g in gt_list}\n",
    "        ndcg_score = 0.0\n",
    "\n",
    "        for rec in rec_list:\n",
    "            gt = gt_dict[rec[\"id\"]]\n",
    "            ndcg_score += self._ndcg(gt[\"items\"], rec[\"items\"])\n",
    "\n",
    "\n",
    "        ndcg_score = ndcg_score / len(rec_list)\n",
    "        ent = self._entropy_diversity(rec_list)\n",
    "        \n",
    "        return ndcg_score, ent\n",
    "\n",
    "    def evaluate(self, gt_list, rec_list):\n",
    "        try:\n",
    "            ndcg_score, ent_score = self._eval(gt_list, rec_list)\n",
    "            print(f\"NDCG: {ndcg_score:.6}\")\n",
    "            print(f\"Entropy Diversity: {ent_score:.6} \")\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53019274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def module(df:pd.DataFrame(), split_date, test_date, all_df)->pd.DataFrame():\n",
    "    \n",
    "    # paid orders만 가져오기\n",
    "    df['date_paid'] = pd.to_datetime(df['date_paid'])\n",
    "    df_only_paid = df[~df['date_paid'].isna()]\n",
    "    # 3개월치 데이터만 가져오기\n",
    "    df_date = df_only_paid[df_only_paid['date_paid'] >= split_date]\n",
    "    # 취소 안된 것만 가져오기\n",
    "    complete_df = df_date[(df_date['paid'] == True) & (df_date['cancelled']==False)]\n",
    "    # 도서 카테고리만 가져오기\n",
    "    only_book = complete_df[complete_df['name'] == '도서']\n",
    "\n",
    "    # 유저가 중복으로 아이템 구매 삭제\n",
    "    df_duplicated_book = only_book.drop_duplicates(subset=['customer_id','product_ids'])\n",
    "    df_book = df_duplicated_book.sort_values(by='date_paid').reset_index(drop=True)\n",
    "    \n",
    "    # medirecommend 만들기\n",
    "    df = df.dropna(subset=['product_ids','name_x'])\n",
    "\n",
    "    # paid orders만 가져오기\n",
    "    df['date_paid'] = pd.to_datetime(df['date_paid'])\n",
    "    df_only_paid = df[~df['date_paid'].isna()]\n",
    "    # 취소 안된 것만 가져오기\n",
    "    complete_df = df_only_paid[(df_only_paid['paid'] == True) & (df_only_paid['cancelled']==False)]\n",
    "    # 도서 카테고리만 가져오기\n",
    "    only_book = complete_df[complete_df['name'] == '도서']\n",
    "\n",
    "    # 유저가 중복으로 아이템 구매 삭제\n",
    "    df_duplicated_book = only_book.drop_duplicates(subset=['customer_id','product_ids'])\n",
    "    df_sort = df_duplicated_book.sort_values(by='date_paid').reset_index(drop=True)\n",
    "    df_sort = product_name_fill(df_sort)\n",
    "    df_sort = df_sort.drop_duplicates(subset=['customer_id','product_ids']).reset_index(drop=True)\n",
    "    \n",
    "    # 변수 처리한 기간 데이터만 가져오기\n",
    "    df_book = df_sort[df_sort['date_paid'] >= split_date].reset_index(drop=True)\n",
    "\n",
    "    # 마지막 cross validation 6주 제외한 medirecommend 만들기\n",
    "    mediprediction_all_df = df_sort[df_sort['date_paid'] < test_date].reset_index(drop=True)\n",
    "    \n",
    "    train_before = df_book[df_book['date_paid'] < test_date]\n",
    "    train = promotion_proprof(train_before)\n",
    "    test = df_book[df_book['date_paid'].dt.date == pd.to_datetime(test_date)]\n",
    "    \n",
    "    \n",
    "    # test 만 있는 item 제거\n",
    "    only_test_items = set(test.product_ids.unique())-set(train.product_ids.unique())\n",
    "    if_prepro_test = test[~test['product_ids'].isin(only_test_items)]\n",
    "    \n",
    "    \n",
    "    PdIds = train.product_ids.unique()\n",
    "\n",
    "    PdIdToIndex = {}\n",
    "    indexToPdId = {}\n",
    "\n",
    "    colIdx = 0\n",
    "\n",
    "    for PdId in PdIds:\n",
    "        PdIdToIndex[PdId] = colIdx\n",
    "        indexToPdId[colIdx] = PdId\n",
    "        colIdx += 1\n",
    "\n",
    "    userIds = train.customer_id.unique()\n",
    "\n",
    "    userIdToIndex = {}\n",
    "    indexToUserId = {}\n",
    "\n",
    "    rowIdx = 0\n",
    "\n",
    "    for userId in userIds:\n",
    "        userIdToIndex[userId] = rowIdx\n",
    "        indexToUserId[rowIdx] = userId\n",
    "        rowIdx += 1\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "\n",
    "    for row in train.itertuples():\n",
    "        rows.append(userIdToIndex[row.customer_id])\n",
    "        cols.append(PdIdToIndex[row.product_ids])\n",
    "        vals.append(1)\n",
    "\n",
    "    purchase_sparse = sp.csr_matrix((vals, (rows, cols)), shape=(rowIdx,colIdx))\n",
    "\n",
    "    matrix = purchase_sparse.todense()\n",
    "\n",
    "    medistream_prediction_df = mediprediction_all_df[['date_created','regular_price','sale_price','three_months','product_ids','name_x']]\n",
    "    medistream_prediction_preprop_df = medistream_prediction_df.drop_duplicates(subset=['product_ids'], ignore_index=True)\n",
    "    medistream_prediction_preprop_df['date_created'] = pd.to_datetime(medistream_prediction_preprop_df['date_created'])    \n",
    "    \n",
    "    most_popular = mediprediction_all_df.groupby(['product_ids']).count()['customer_id'].reset_index()\n",
    "    most_popular_list = most_popular.sort_values(by='customer_id',ascending=False).index\n",
    "    \n",
    "\n",
    "        # test 예측값, 이미 구매 했을 경우 제외\n",
    "    als_predict_list = []\n",
    "    for user_id in test['customer_id'].unique():\n",
    "        try:\n",
    "            result = als_model.recommend(userIdToIndex[user_id], purchase_sparse[userIdToIndex[user_id]], N=15)\n",
    "            als_predict_list.append({'id':user_id ,'items':[indexToPdId[num] for num in result[0]]})\n",
    "        except:\n",
    "            train_purchase_list = list(train[train['customer_id']==user_id].product_ids)\n",
    "            als_predict_list.append({'id':user_id ,'items':[most_popular.product_ids.loc[num] for num in most_popular_list \\\n",
    "                                                                if most_popular.product_ids.loc[num] not in train_purchase_list \\\n",
    "                                                                ]})\n",
    "\n",
    "        # 15 개만 예측하기\n",
    "    for idx, pred_list in enumerate(als_predict_list):\n",
    "        als_predict_list[idx]['items'] = pred_list['items'][:15]\n",
    "\n",
    "    \n",
    "\n",
    "        # test 예측값\n",
    "    lmf_predict_list = []\n",
    "    for user_id in test['customer_id'].unique():\n",
    "        try:\n",
    "            result = lmf_model.recommend(userIdToIndex[user_id], purchase_sparse[userIdToIndex[user_id]], N=15)\n",
    "            lmf_predict_list.append({'id':user_id ,'items':[indexToPdId[num] for num in result[0]]})\n",
    "        except:\n",
    "            train_purchase_list = list(train[train['customer_id']==user_id].product_ids)\n",
    "            lmf_predict_list.append({'id':user_id ,'items':[most_popular.product_ids.loc[num] for num in most_popular_list \\\n",
    "                                                                if most_popular.product_ids.loc[num] not in train_purchase_list \\\n",
    "                                                                ]})\n",
    "\n",
    "        # 15 개만 예측하기\n",
    "    for idx, pred_list in enumerate(lmf_predict_list):\n",
    "        lmf_predict_list[idx]['items'] = pred_list['items'][:15]\n",
    "\n",
    "    \n",
    "\n",
    "        # test 예측값, 이미 구매 했을 경우 제외\n",
    "    predict_popular_list = []\n",
    "    for user_id in test['customer_id'].unique():\n",
    "        train_purchase_list = list(train[train['customer_id']==user_id].product_ids)\n",
    "        predict_popular_list.append({'id':user_id ,'items':[most_popular.product_ids.loc[num] for num in most_popular_list \\\n",
    "                                                                if most_popular.product_ids.loc[num] not in train_purchase_list \\\n",
    "                                                                ]})\n",
    "\n",
    "        # 15 개만 예측하기\n",
    "    for idx, pred_list in enumerate(predict_popular_list):\n",
    "        predict_popular_list[idx]['items'] = pred_list['items'][:15]\n",
    "        \n",
    "    # real test \n",
    "    ground_trues = []\n",
    "    for user_id in test['customer_id'].unique():\n",
    "        ground_trues.append({'id': user_id,\\\n",
    "        'items':list(test[test['customer_id']==user_id].product_ids)\n",
    "        })\n",
    "\n",
    "    # MP\n",
    "    evaluator = CustomEvaluator()\n",
    "    mp = evaluator._eval(ground_trues, predict_popular_list)\n",
    "    \n",
    "    # 인기도순\n",
    "    medistream_popular_list = medistream_prediction_preprop_df.sort_values(by='three_months', ascending=False).index\n",
    "    # 최신순\n",
    "    medistream_latest_list = medistream_prediction_preprop_df.sort_values(by='date_created', ascending=False).index\n",
    "    # 오랜된 순\n",
    "    medistream_oldest_list = medistream_prediction_preprop_df.sort_values(by='date_created', ascending=True).index\n",
    "    # 높은 가격 순\n",
    "    medistream_high_price_list = medistream_prediction_preprop_df.sort_values(by='sale_price', ascending=False).index\n",
    "    # 낮은 가격 순\n",
    "    medistream_low_price_list = medistream_prediction_preprop_df.sort_values(by='sale_price', ascending=True).index\n",
    "    # 이름 순\n",
    "    medistream_name_sort_list = medistream_prediction_preprop_df.sort_values(by='name_x',ascending=True).index\n",
    "\n",
    "    def medistream_prediction_method(predict_num:int ,medi_predict_list:list)->list:\n",
    "        medistream_predict_list = []\n",
    "        for user_id in test['customer_id'].unique():\n",
    "            medistream_predict_list.append({'id':user_id ,'items':[medistream_prediction_preprop_df.product_ids.loc[num] \\\n",
    "                                                                           for num in medi_predict_list]})\n",
    "\n",
    "        # 15 개만 예측하기\n",
    "        for idx, pred_list in enumerate(medistream_predict_list):\n",
    "            medistream_predict_list[idx]['items'] = pred_list['items'][:predict_num]\n",
    "\n",
    "        return medistream_predict_list\n",
    "    \n",
    "    medistream_predict_popular_list = medistream_prediction_method(15, medistream_popular_list)\n",
    "    medistream_predict_latest_list = medistream_prediction_method(15, medistream_latest_list)\n",
    "    medistream_predict_oldest_list = medistream_prediction_method(15, medistream_oldest_list)\n",
    "    medistream_predict_high_price_list = medistream_prediction_method(15, medistream_high_price_list)\n",
    "    medistream_predict_low_price_list = medistream_prediction_method(15, medistream_low_price_list)\n",
    "    medistream_predict_name_sort_list = medistream_prediction_method(15, medistream_name_sort_list)\n",
    "    \n",
    "    def medistream_prediction(ground_trues:list, predict_list:list):\n",
    "        evaluator = CustomEvaluator()\n",
    "        ndcg, entropy = evaluator._eval(ground_trues, predict_list)\n",
    "\n",
    "        assert len(predict_list) == len(ground_trues)\n",
    "\n",
    "        cnt = 0\n",
    "        for gt, pred_list in zip(ground_trues, predict_list):\n",
    "            for pred in pred_list['items']:\n",
    "                if pred in gt['items']:\n",
    "                    cnt += 1\n",
    "        return ndcg, entropy, cnt\n",
    "    \n",
    "    medistream_predict_score = {'medistream_predict':['medi_popular','latest','oldest','high_price','low_price','name_sort'], \\\n",
    "                                'ndcg':[], 'entropy':[], 'cnt':[]}\n",
    "\n",
    "    medistream_predict_list = [medistream_predict_popular_list, medistream_predict_latest_list, medistream_predict_oldest_list,\\\n",
    "                              medistream_predict_high_price_list, medistream_predict_low_price_list, medistream_predict_name_sort_list]\n",
    "\n",
    "    for medistream_predict in medistream_predict_list:\n",
    "        ndcg, entropy, cnt = medistream_prediction(ground_trues, medistream_predict)\n",
    "        medistream_predict_score['ndcg'].append(ndcg)\n",
    "        medistream_predict_score['entropy'].append(entropy)\n",
    "        medistream_predict_score['cnt'].append(cnt)\n",
    "\n",
    "        \n",
    "######## hyper parameter\n",
    "    als_mf_hyper_parameter = {'factor':[],'regularization':[],'iteration':[],'NDCG':[],'entropy':[]}\n",
    "\n",
    "    factors = [5]\n",
    "    regularizations = [0.01]\n",
    "    iterations = [5]\n",
    "\n",
    "    for factor in factors:\n",
    "        for regularization in regularizations:\n",
    "            for iteration in iterations:\n",
    "                als_model = ALS(factors=factor, regularization=regularization, iterations = iteration, random_state=42)\n",
    "                als_model.fit(purchase_sparse, show_progress=False)\n",
    "\n",
    "                # 신규 유저인 경우 mp로 넣기\n",
    "                # 전체 도서에 대한 판매 만큼 정렬 후 넣기\n",
    "                most_popular_list = most_popular.sort_values(by='customer_id',ascending=False).index\n",
    "\n",
    "                # test 예측값, 이미 구매 했을 경우 제외\n",
    "                als_predict_list = []\n",
    "                for user_id in test['customer_id'].unique():\n",
    "                    try:\n",
    "                        result = als_model.recommend(userIdToIndex[user_id], purchase_sparse[userIdToIndex[user_id]], N=15)\n",
    "                        als_predict_list.append({'id':user_id ,'items':[indexToPdId[num] for num in result[0]]})\n",
    "                    except:\n",
    "                        train_purchase_list = list(train[train['customer_id']==user_id].product_ids)\n",
    "                        als_predict_list.append({'id':user_id ,'items':[most_popular.product_ids.loc[num] for num in most_popular_list \\\n",
    "                                                                            if most_popular.product_ids.loc[num] not in train_purchase_list \\\n",
    "                                                                            ]})\n",
    "\n",
    "                # 15 개만 예측하기\n",
    "                for idx, pred_list in enumerate(als_predict_list):\n",
    "                    als_predict_list[idx]['items'] = pred_list['items'][:15]\n",
    "\n",
    "                # ALS \n",
    "                evaluator = CustomEvaluator()\n",
    "                ndcg, entropy = evaluator._eval(ground_trues, als_predict_list)\n",
    "\n",
    "                als_mf_hyper_parameter['factor'].append(factor)\n",
    "                als_mf_hyper_parameter['regularization'].append(regularization)\n",
    "                als_mf_hyper_parameter['iteration'].append(iteration)\n",
    "                als_mf_hyper_parameter['NDCG'].append(ndcg)\n",
    "                als_mf_hyper_parameter['entropy'].append(entropy)\n",
    "                \n",
    "                \n",
    "    lmf_hyper_parameter = {'factor':[],'regularization':[],'iteration':[],'NDCG':[],'entropy':[]}\n",
    "\n",
    "    factors = [15]\n",
    "    regularizations = [0.005]\n",
    "    iterations = [50]\n",
    "\n",
    "    for factor in factors:\n",
    "        for regularization in regularizations:\n",
    "            for iteration in iterations:\n",
    "                lmf_model = LMF(factors=factor, regularization=regularization, iterations = iteration, random_state=42)\n",
    "                lmf_model.fit(purchase_sparse, show_progress=False)\n",
    "\n",
    "                # 신규 유저 mp로 넣기\n",
    "                most_popular_list = most_popular.sort_values(by='customer_id',ascending=False).index\n",
    "\n",
    "                # test 예측값\n",
    "                lmf_predict_list = []\n",
    "                for user_id in test['customer_id'].unique():\n",
    "                    try:\n",
    "                        result = lmf_model.recommend(userIdToIndex[user_id], purchase_sparse[userIdToIndex[user_id]], N=15)\n",
    "                        lmf_predict_list.append({'id':user_id ,'items':[indexToPdId[num] for num in result[0]]})\n",
    "                    except:\n",
    "                        train_purchase_list = list(train[train['customer_id']==user_id].product_ids)\n",
    "                        lmf_predict_list.append({'id':user_id ,'items':[most_popular.product_ids.loc[num] for num in most_popular_list \\\n",
    "                                                                            if most_popular.product_ids.loc[num] not in train_purchase_list \\\n",
    "                                                                            ]})\n",
    "\n",
    "                # 15 개만 예측하기\n",
    "                for idx, pred_list in enumerate(lmf_predict_list):\n",
    "                    lmf_predict_list[idx]['items'] = pred_list['items'][:15]\n",
    "\n",
    "                # LMF\n",
    "                evaluator = CustomEvaluator()\n",
    "                ndcg, entropy = evaluator._eval(ground_trues, lmf_predict_list)\n",
    "\n",
    "                lmf_hyper_parameter['factor'].append(factor)\n",
    "                lmf_hyper_parameter['regularization'].append(regularization)\n",
    "                lmf_hyper_parameter['iteration'].append(iteration)\n",
    "                lmf_hyper_parameter['NDCG'].append(ndcg)\n",
    "                lmf_hyper_parameter['entropy'].append(entropy)\n",
    "\n",
    "                \n",
    "    medipop_lmf_mix_hyper_parameter = {'factor':[],'regularization':[],'iteration':[],'top':[],'NDCG':[],'entropy':[]}\n",
    "\n",
    "    factors = [40]\n",
    "    regularizations = [0.005]\n",
    "    iterations = [50]\n",
    "    tops = [3]\n",
    "\n",
    "    for factor in factors:\n",
    "        for regularization in regularizations:\n",
    "            for iteration in iterations:\n",
    "                for top in tops:\n",
    "                    lmf_model = LMF(factors=factor, regularization=regularization, iterations = iteration, random_state=42)\n",
    "                    lmf_model.fit(purchase_sparse, show_progress=False)\n",
    "\n",
    "                    # 신규 유저 mp로 넣기\n",
    "                    most_popular_list = most_popular.sort_values(by='customer_id',ascending=False).index\n",
    "\n",
    "                    # test 예측값\n",
    "                    lmf_predict_list = []\n",
    "                    for user_id in test['customer_id'].unique():\n",
    "                        try:\n",
    "                            train_purchase_list = list(train[train['customer_id']==user_id].product_ids)\n",
    "                            medi_popular_top_three = medistream_popular_list[:top]\n",
    "                            medi_popular_top_three_list = [medistream_prediction_preprop_df.product_ids.loc[num] for num in medi_popular_top_three \\\n",
    "                                                                                if medistream_prediction_preprop_df.product_ids.loc[num] not in train_purchase_list \\\n",
    "                                                                                ]\n",
    "                            result = lmf_model.recommend(userIdToIndex[user_id], purchase_sparse[userIdToIndex[user_id]], N=20)\n",
    "                            result_list = [indexToPdId[num] for num in result[0]]\n",
    "                            medi_pop_lmf_list = list(dict.fromkeys(medi_popular_top_three_list + result_list))\n",
    "                            lmf_predict_list.append({'id':user_id ,'items':medi_pop_lmf_list})\n",
    "                        except:\n",
    "                            train_purchase_list = list(train[train['customer_id']==user_id].product_ids)\n",
    "                            lmf_predict_list.append({'id':user_id ,'items':[medistream_prediction_preprop_df.product_ids.loc[num] for num in medistream_popular_list \\\n",
    "                                                                                if medistream_prediction_preprop_df.product_ids.loc[num] not in train_purchase_list \\\n",
    "                                                                                ]})\n",
    "\n",
    "                    # 15 개만 예측하기\n",
    "                    for idx, pred_list in enumerate(lmf_predict_list):\n",
    "                        lmf_predict_list[idx]['items'] = pred_list['items'][:15]\n",
    "\n",
    "                    # LMF\n",
    "                    evaluator = CustomEvaluator()\n",
    "                    ndcg, entropy = evaluator._eval(ground_trues, lmf_predict_list)\n",
    "\n",
    "                    medipop_lmf_mix_hyper_parameter['factor'].append(factor)\n",
    "                    medipop_lmf_mix_hyper_parameter['regularization'].append(regularization)\n",
    "                    medipop_lmf_mix_hyper_parameter['iteration'].append(iteration)\n",
    "                    medipop_lmf_mix_hyper_parameter['top'].append(top)\n",
    "                    medipop_lmf_mix_hyper_parameter['NDCG'].append(ndcg)\n",
    "                    medipop_lmf_mix_hyper_parameter['entropy'].append(entropy)\n",
    "                \n",
    "#     return als_mf_hyper_parameter, lmf_hyper_parameter, medistream_predict_score\n",
    "\n",
    "    all_prediction_df = {'first_day':[],'last_day':[],'train_데이터수':[],'train_유저수':[],'test_데이터수':[],\\\n",
    "        'test_유저수':[],'test_신규유저수':[],'test_신규아이템수':[],'원본_test수':[],'전처리진행test수':[],\\\n",
    "        'als_mf':[],'lmf':[],'medi_mp_lmf_mix':[],'mp':[],'medi_popular':[],'latest':[],\\\n",
    "        'oldest':[],'high_price':[],'low_price':[],'name_sort':[],\\\n",
    "         'als_mf_entropy':[],'lmf_entropy':[],'medi_mp_lmf_mix_entropy':[],'mp_entropy':[],'medi_popular_entropy':[],'latest_entropy':[],\\\n",
    "         'oldest_entropy':[],'high_price_entropy':[],'low_price_entropy':[],'name_sort_entropy':[]}\n",
    "    medistream_predict_df = pd.DataFrame(medistream_predict_score)\n",
    "\n",
    "    all_prediction_df['first_day'].append(str(datetime.date(train['date_paid'].min()))+' '+str(datetime.date(train['date_paid'].max())))\n",
    "    all_prediction_df['last_day'].append(str(datetime.date(test['date_paid'].min()))+' '+str(datetime.date(test['date_paid'].max())))\n",
    "    all_prediction_df['train_데이터수'].append(len(train))\n",
    "    all_prediction_df['train_유저수'].append(len(set(train.customer_id)))\n",
    "    all_prediction_df['test_데이터수'].append(len(test))\n",
    "    all_prediction_df['test_유저수'].append(len(set(test.customer_id)))\n",
    "    all_prediction_df['test_신규유저수'].append(len(set(test['customer_id'].unique())- set(train['customer_id'].unique())))\n",
    "    all_prediction_df['test_신규아이템수'].append(len(set(test.product_ids.unique())-set(train.product_ids.unique())))\n",
    "    all_prediction_df['원본_test수'].append(len(test))\n",
    "    all_prediction_df['전처리진행test수'].append(len(if_prepro_test))\n",
    "\n",
    "    # ndcg\n",
    "    all_prediction_df['als_mf'].append(pd.DataFrame(als_mf_hyper_parameter).sort_values(by='NDCG',ascending=False)['NDCG'].iloc[0])\n",
    "    all_prediction_df['lmf'].append(pd.DataFrame(lmf_hyper_parameter).sort_values(by='NDCG',ascending=False)['NDCG'].iloc[0])\n",
    "    all_prediction_df['medi_mp_lmf_mix'].append(pd.DataFrame(medipop_lmf_mix_hyper_parameter).sort_values(by='NDCG',ascending=False)['NDCG'].iloc[0])\n",
    "    all_prediction_df['mp'].append(evaluator._eval(ground_trues, predict_popular_list)[0])\n",
    "    all_prediction_df['medi_popular'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='medi_popular'].iloc[0]['ndcg'])\n",
    "    all_prediction_df['latest'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='latest'].iloc[0]['ndcg'])\n",
    "    all_prediction_df['oldest'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='oldest'].iloc[0]['ndcg'])\n",
    "    all_prediction_df['high_price'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='high_price'].iloc[0]['ndcg'])\n",
    "    all_prediction_df['low_price'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='low_price'].iloc[0]['ndcg'])\n",
    "    all_prediction_df['name_sort'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='name_sort'].iloc[0]['ndcg'])\n",
    "\n",
    "#     entropy\n",
    "    all_prediction_df['als_mf_entropy'].append(pd.DataFrame(als_mf_hyper_parameter).sort_values(by='entropy',ascending=False)['entropy'].iloc[0])\n",
    "    all_prediction_df['lmf_entropy'].append(pd.DataFrame(lmf_hyper_parameter).sort_values(by='entropy',ascending=False)['entropy'].iloc[0])\n",
    "    all_prediction_df['medi_mp_lmf_mix_entropy'].append(pd.DataFrame(medipop_lmf_mix_hyper_parameter).sort_values(by='entropy',ascending=False)['entropy'].iloc[0])\n",
    "    all_prediction_df['mp_entropy'].append(evaluator._eval(ground_trues, predict_popular_list)[1])\n",
    "    all_prediction_df['medi_popular_entropy'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='medi_popular'].iloc[0]['entropy'])\n",
    "    all_prediction_df['latest_entropy'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='latest'].iloc[0]['entropy'])\n",
    "    all_prediction_df['oldest_entropy'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='oldest'].iloc[0]['entropy'])\n",
    "    all_prediction_df['high_price_entropy'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='high_price'].iloc[0]['entropy'])\n",
    "    all_prediction_df['low_price_entropy'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='low_price'].iloc[0]['entropy'])\n",
    "    all_prediction_df['name_sort_entropy'].append(medistream_predict_df[medistream_predict_df['medistream_predict']=='name_sort'].iloc[0]['entropy'])\n",
    "\n",
    "    print('train 총 기간:',train['date_paid'].max()-train['date_paid'].min())\n",
    "    print('test 총 기간:',test['date_paid'].max()-test['date_paid'].min())\n",
    "    \n",
    "    return pd.DataFrame(all_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7e20c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "twenty_df_list = []\n",
    "for sp_day, tt_day in tqdm(zip(split_day_list,test_day_list)):\n",
    "    module_df = module(df, sp_day,tt_day, all_df)\n",
    "    twenty_df_list.append(module_df)\n",
    "twenty_df = pd.concat(twenty_df_list, ignore_index=True)\n",
    "twenty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7dbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
